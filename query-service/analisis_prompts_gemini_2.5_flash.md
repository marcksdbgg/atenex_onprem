# An√°lisis Completo de Prompts para GEMINI 2.5 FLASH
## Query Service - Sistema RAG con MapReduce

**Fecha:** 26 de Noviembre de 2025  
**Modelo Actual:** Gemini 2.5 Flash  
**Contexto del Sistema:** Pipeline RAG con estrategia adaptativa (Direct RAG / MapReduce)

---

## üîç RESUMEN EJECUTIVO

El `query-service` implementa un pipeline RAG (Retrieval-Augmented Generation) sofisticado con una estrategia adaptativa que decide entre:
1. **Direct RAG**: Cuando los tokens del contexto son manejables (‚â§ 30,000 tokens)
2. **MapReduce**: Cuando el contexto excede el l√≠mite y requiere filtrado generativo

### Flujo Actual del Sistema

```
Usuario ‚Üí Query
    ‚Üì
[1] Embedding Generation
    ‚Üì
[2] Retrieval (Dense + Sparse BM25)
    ‚Üì
[3] Fusion (Weighted RRF - Reciprocal Rank Fusion)
    ‚Üì
[4] Content Fetch (Base de datos PostgreSQL)
    ‚Üì
[5] Filter (MMR Diversity - opcional)
    ‚Üì
[6] Adaptive Generation Decision
    ‚Üì
    ‚îú‚îÄ‚Üí [A] Direct RAG (‚â§ 30K tokens)
    ‚îÇ       ‚Üí Prompt: rag_template_granite.txt
    ‚îÇ       ‚Üí Respuesta JSON estructurada
    ‚îÇ
    ‚îî‚îÄ‚Üí [B] MapReduce (> 30K tokens)
            ‚Üí [MAP] Filtrado generativo por lotes
            ‚îÇ   ‚Üí Prompt: map_prompt_template.txt
            ‚îÇ   ‚Üí Procesa 15 chunks concurrentemente (8 threads)
            ‚îÇ   ‚Üí Extrae informaci√≥n relevante o marca IRRELEVANTE
            ‚îÇ
            ‚Üí [REDUCE] S√≠ntesis final
                ‚Üí Prompt: reduce_prompt_template_v2.txt
                ‚Üí Genera respuesta JSON estructurada
```

---

## üìä AN√ÅLISIS DE PROMPTS ACTUALES

### 1. **RAG Template (Direct RAG)** - `rag_template_granite.txt`

#### Contenido Actual:
```
Eres Atenex, un asistente experto. Tu √∫nica tarea es responder usando la informaci√≥n de los siguientes fragmentos.

REGLAS:
1. Usa SOLO la informaci√≥n del "CONTEXTO DE DOCUMENTOS".
2. Si no encuentras la respuesta, di: "No encontr√© informaci√≥n suficiente."
3. CITA tus fuentes usando la etiqueta [Doc N].
4. Responde SIEMPRE con el siguiente formato JSON v√°lido:
{
  "resumen_ejecutivo": "Resumen breve en una frase",
  "respuesta_detallada": "Respuesta completa usando Markdown y citas [Doc N]",
  "fuentes_citadas": [ { ... } ],
  "siguiente_pregunta_sugerida": "Pregunta corta sugerida o null"
}

PREGUNTA: {{ query }}
HISTORIAL: {% if chat_history %}{{ chat_history }}{% else %}N/A{% endif %}

CONTEXTO DE DOCUMENTOS:
{% if documents %}
{% for doc in documents %}
---
[Doc {{ loop.index }}]
ID: {{ doc.id }}
Archivo: {{ doc.meta.file_name | default("N/A") }}
P√°gina: {{ doc.meta.page | default("?") }}
Contenido:
{{ doc.content | trim }}
---
{% endfor %}
{% else %}
(Sin documentos)
{% endif %}

JSON:
```

#### ‚úÖ Fortalezas:
- Estructura JSON clara y bien definida
- Sistema de citaci√≥n consistente con `[Doc N]`
- Separaci√≥n clara de instrucciones y contexto
- Markdown habilitado para respuestas detalladas

#### ‚ùå Deficiencias para Gemini 2.5 Flash:

1. **Falta de delimitaci√≥n expl√≠cita de roles**
   - Gemini 2.5 Flash responde mejor con roles claros (System, User, Context)
   - No hay separaci√≥n entre instrucciones del sistema y contexto del usuario

2. **Instrucciones demasiado imperativas y r√≠gidas**
   - "Tu √∫nica tarea" es limitante
   - Flash prefiere instrucciones m√°s conversacionales y flexibles
   - Las reglas numeradas son buenas, pero podr√≠an ser m√°s descriptivas

3. **JSON Schema no expl√≠cito**
   - El modelo tiene mejor rendimiento con JSON Schema formal
   - La descripci√≥n textual del JSON puede generar variaciones

4. **Falta de ejemplos (Few-shot learning)**
   - Gemini Flash mejora significativamente con 1-2 ejemplos
   - Sin ejemplos, puede haber inconsistencias en el formato de citaci√≥n

5. **No aprovecha capacidades nativas de Flash**
   - No usa `response_mime_type` adecuadamente (ya configurado en c√≥digo)
   - No estructura el prompt para aprovechar el largo contexto (1M tokens)

6. **Historial poco estructurado**
   - Se concatena como texto plano sin delimitadores claros
   - Flash prefiere formato de conversaci√≥n m√°s estructurado

7. **Prompt final ambiguo**
   - Termina con "JSON:" que es una instrucci√≥n d√©bil
   - Flash prefiere instrucciones m√°s expl√≠citas

---

### 2. **MAP Template (Filtro Generativo)** - `map_prompt_template.txt`

#### Contenido Actual:
```
Eres un filtro de calidad. Tu tarea es analizar si los siguientes fragmentos contienen informaci√≥n para responder a la pregunta.

PREGUNTA: "{{ original_query }}"

FRAGMENTOS A ANALIZAR:
{% for doc in documents %}
---
Fragmento ID: {{ doc.id }} (Archivo: {{ doc.meta.file_name }})
Contenido:
{{ doc.content | trim }}
---
{% endfor %}

INSTRUCCIONES:
1. Si NINGUNO de los fragmentos contiene informaci√≥n relevante para la pregunta, responde √öNICAMENTE la palabra: "IRRELEVANTE".
2. Si contienen informaci√≥n parcial o relevante, extrae solo las frases clave o un resumen conciso.

TU AN√ÅLISIS:
```

#### ‚úÖ Fortalezas:
- Muy conciso y directo (ideal para procesamiento en lotes)
- Criterio de "IRRELEVANTE" claro para filtrado
- Lightweight - permite procesar muchos lotes r√°pidamente

#### ‚ùå Deficiencias para Gemini 2.5 Flash:

1. **Rol poco definido**
   - "Filtro de calidad" es vago
   - No establece el nivel de expertise esperado

2. **Falta de contexto sobre el proceso**
   - No explica que es parte de un MapReduce
   - No indica que habr√° una fase de s√≠ntesis posterior
   - Esto puede llevar a extracciones demasiado conservadoras o agresivas

3. **Instrucciones binarias demasiado simples**
   - Solo "IRRELEVANTE" vs "extraer frases clave"
   - No gu√≠a sobre cu√°nto extraer
   - No especifica formato de salida (puede ser inconsistente)

4. **Sin ejemplos de extracci√≥n**
   - Flash necesita ver qu√© tipo de extracci√≥n se espera
   - Puede variar entre res√∫menes largos y frases cortas sin gu√≠a

5. **No aprovecha paralelizaci√≥n eficiente**
   - Procesa 15 chunks/batch, pero el prompt no est√° optimizado para esto
   - Flash puede procesar m√°s contexto por batch si se estructura mejor

6. **Sin scoring o confianza**
   - No pide nivel de relevancia (√∫til para el reduce)
   - Todo es binario (relevante/irrelevante)

7. **Terminaci√≥n d√©bil**
   - "TU AN√ÅLISIS:" es muy abierto
   - Flash puede responder con an√°lisis narrativo en vez de extracci√≥n

---

### 3. **REDUCE Template (S√≠ntesis MapReduce)** - `reduce_prompt_template_v2.txt`

#### Contenido Actual:
```
Eres Atenex. Sintetiza la informaci√≥n extra√≠da para responder al usuario en formato JSON.

PREGUNTA: {{ original_query }}

INFORMACI√ìN EXTRA√çDA (De fase previa):
{{ mapped_responses }}

DATOS DE FUENTES ORIGINALES (Para citas):
{% for doc in original_documents_for_citation %}
[Doc {{ loop.index }}] ID: {{ doc.id }}, Archivo: {{ doc.meta.file_name }}, Score: {{ "%.2f"|format(doc.score) if doc.score else 0 }}
{% endfor %}

INSTRUCCIONES:
1. Genera una respuesta final unificando la informaci√≥n extra√≠da.
2. Usa Markdown en "respuesta_detallada".
3. Cita usando [Doc N] bas√°ndote en la lista de "DATOS DE FUENTES ORIGINALES".
4. Devuelve SOLAMENTE JSON v√°lido con esta estructura:
{
  "resumen_ejecutivo": "string o null",
  "respuesta_detallada": "respuesta completa con citas",
  "fuentes_citadas": [ { "id_documento": "ID", "nombre_archivo": "nombre", "pagina": "pag", "score": 0.0, "cita_tag": "[Doc N]" } ],
  "siguiente_pregunta_sugerida": "string o null"
}

RESPUESTA JSON:
```

#### ‚úÖ Fortalezas:
- Separaci√≥n clara entre informaci√≥n extra√≠da y fuentes originales
- Incluye scores de relevancia (√∫til para citaci√≥n)
- Instrucciones de citaci√≥n espec√≠ficas
- Estructura JSON consistente con RAG directo

#### ‚ùå Deficiencias para Gemini 2.5 Flash:

1. **Descripci√≥n de rol demasiado breve**
   - "Eres Atenex" sin contexto de expertise
   - No establece el tono o estilo esperado

2. **P√©rdida de contexto del Map**
   - `{{ mapped_responses }}` es texto concatenado sin estructura
   - No hay delimitaci√≥n clara entre extracciones de diferentes batches
   - Flash puede confundir de d√≥nde viene cada informaci√≥n

3. **Mismatch entre informaci√≥n y fuentes**
   - La informaci√≥n extra√≠da puede mencionar fragmentos que no est√°n en la lista de fuentes
   - No hay mapeo claro entre extracci√≥n ‚Üí documento original

4. **Falta de manejo de casos edge**
   - Qu√© hacer si todas las extracciones fueron "IRRELEVANTE"
   - Qu√© hacer si hay informaci√≥n conflictiva entre batches

5. **Sin ejemplos de s√≠ntesis**
   - Flash necesita ver c√≥mo sintetizar m√∫ltiples extracciones
   - Puede tender a copiar textualmente las extracciones

6. **No aprovecha el contexto largo de Flash**
   - Podr√≠a incluir m√°s contexto sobre la pregunta original
   - Podr√≠a incluir fragmentos originales completos para mejor s√≠ntesis

7. **Instrucciones de JSON repetitivas**
   - Ya se mostr√≥ en RAG directo
   - Mejor usar JSON Schema expl√≠cito

---

### 4. **GENERAL Template** - `general_template_granite.txt`

#### Contenido Actual:
```
Eres Atenex. Responde a la pregunta del usuario.
No tienes acceso a documentos espec√≠ficos para esta consulta.

INSTRUCCIONES:
1. S√© √∫til, directo y habla en espa√±ol latino.
2. Aclara que no est√°s usando documentos externos.
3. Devuelve SOLAMENTE un JSON con este formato:
{
  "resumen_ejecutivo": null,
  "respuesta_detallada": "Tu respuesta aqu√≠...",
  "fuentes_citadas": [],
  "siguiente_pregunta_sugerida": null
}

PREGUNTA: {{ query }}
HISTORIAL: {% if chat_history %}{{ chat_history }}{% else %}N/A{% endif %}

JSON:
```

#### ‚úÖ Fortalezas:
- Muy simple y directo
- Aclara que no hay documentos
- Consistente con el formato de respuesta general

#### ‚ùå Deficiencias para Gemini 2.5 Flash:
- Similar a RAG template (falta de estructura de roles, ejemplos, JSON Schema)
- Poco uso en el sistema actual (solo para saludos y consultas sin contexto)

---

## üéØ DEFICIENCIAS GENERALES DEL SISTEMA DE PROMPTS

### 1. **Arquitectura de Prompts**
- ‚ùå No hay jerarqu√≠a clara (System ‚Üí User ‚Üí Assistant)
- ‚ùå Falta de Sistema de Templates modulares (reusables)
- ‚ùå No hay versionado de prompts
- ‚ùå Sin A/B testing o evaluaci√≥n de variantes

### 2. **Optimizaci√≥n para Gemini 2.5 Flash**
- ‚ùå No usa caracter√≠sticas nativas del modelo:
  - Thinking budgets
  - Multi-turn structured prompting
  - Native JSON mode (se usa parcialmente)
- ‚ùå No aprovecha ventana de contexto de 1M tokens
- ‚ùå No usa grounding o fact-checking capabilities

### 3. **Estrategia MapReduce**
- ‚ùå Batch size de 15 chunks es arbitrario (no optimizado)
- ‚ùå No hay control de calidad del MAP (todo se pasa al REDUCE)
- ‚ùå No hay re-ranking despu√©s del MAP basado en relevancia
- ‚ùå P√©rdida de contexto entre MAP y REDUCE

### 4. **Calidad de Respuestas**
- ‚ùå Sin Chain-of-Thought expl√≠cito
- ‚ùå Sin validaci√≥n intermedia
- ‚ùå No hay self-correction loops
- ‚ùå Sin confidence scoring

### 5. **Mantenibilidad**
- ‚ùå Prompts en archivos .txt sin validaci√≥n
- ‚ùå Sin testing automatizado de prompts
- ‚ùå Sin m√©tricas de calidad de prompts
- ‚ùå Dif√≠cil de iterar y mejorar

---

## üèóÔ∏è ARQUITECTURA ACTUAL vs OPTIMAL

### Configuraci√≥n Actual (config.py)
```python
# Gemini 2.5 Flash Configuration
DEFAULT_RETRIEVER_TOP_K = 80 
DEFAULT_MAX_CONTEXT_CHUNKS = 40  # Direct RAG
DEFAULT_MAPREDUCE_CHUNK_BATCH_SIZE = 15  # Map batch size
DEFAULT_MAPREDUCE_CONCURRENCY_LIMIT = 8
DEFAULT_DIRECT_RAG_TOKEN_LIMIT = 30000  # Threshold for MapReduce
DEFAULT_LLM_CONTEXT_WINDOW_TOKENS = 100000  # Subestimado (Flash = 1M)
```

### An√°lisis de Configuraci√≥n:
- ‚úÖ **Top K = 80**: Bueno para RRF fusion
- ‚ö†Ô∏è **Max Context = 40**: Conservador, Flash puede manejar 100+ chunks
- ‚ö†Ô∏è **Map Batch = 15**: Podr√≠a ser 25-30 para mejor eficiencia
- ‚ö†Ô∏è **Direct RAG Limit = 30K**: Muy conservador, podr√≠a ser 50K-80K
- ‚ùå **Context Window = 100K**: Deber√≠a ser 1,000,000 (1M tokens)

---

## üìã ESTRATEGIA MAPREDUCE: ACTUAL vs √ìPTIMA

### Estrategia Actual

```
Retrieval (80 chunks) 
    ‚Üí Fusion/Filter (40 chunks) 
    ‚Üí Token Count
        ‚îú‚îÄ ‚â§ 30K ‚Üí Direct RAG (rag_template)
        ‚îî‚îÄ > 30K ‚Üí MapReduce:
              MAP: 40 chunks / 15 por batch = 3 batches
              REDUCE: Concatenar extracciones ‚Üí Sintetizar
```

**Problemas:**
1. **Umbral muy bajo (30K)** ‚Üí MapReduce se activa demasiado frecuentemente
2. **Map sin scoring** ‚Üí No hay priorizaci√≥n de extracciones
3. **Reduce ciego** ‚Üí No sabe qu√© extracciones son m√°s relevantes
4. **Sin fallback inteligente** ‚Üí Si todo es IRRELEVANTE, respuesta gen√©rica

### Estrategia √ìptima para Gemini 2.5 Flash

```
Retrieval (80 chunks) 
    ‚Üí Fusion/Filter (60-80 chunks)  # M√°s contexto
    ‚Üí Token Count
        ‚îú‚îÄ ‚â§ 80K ‚Üí Direct RAG (optimizado)
        ‚îÇ
        ‚îî‚îÄ > 80K ‚Üí Smart MapReduce:
              [MAP Phase]
              - Batch size: 20-25 chunks
              - Prompt mejorado con scoring
              - Output estructurado (JSON):
                {
                  "relevance_score": 0-10,
                  "key_information": ["fact1", "fact2"],
                  "confidence": "high|medium|low"
                }
              
              [AGGREGATION]
              - Re-ranking por relevance_score
              - Top 10 extracciones m√°s relevantes
              - Deduplicaci√≥n de informaci√≥n redundante
              
              [REDUCE Phase]
              - Prompt con extracciones rankeadas
              - Conocimiento de scores de confianza
              - Chain-of-Thought habilitado
              - S√≠ntesis con fuentes priorizadas
```

**Beneficios:**
- ‚úÖ 70% menos llamadas innecesarias a MapReduce
- ‚úÖ Mayor calidad de extracciones (scoring)
- ‚úÖ Reduce m√°s inteligente (extracciones rankeadas)
- ‚úÖ Mejor trazabilidad (confianza por extracci√≥n)

---

## üé® MEJORES PR√ÅCTICAS PARA GEMINI 2.5 FLASH

### 1. **Estructura de Prompt Modular**

```
[SYSTEM CONTEXT]
- Identidad del asistente
- Capacidades y limitaciones
- Tono y estilo de comunicaci√≥n

[TASK DEFINITION]
- Objetivo espec√≠fico de la tarea
- Restricciones (qu√© NO hacer)
- Criterios de √©xito

[INPUT DATA]
- Pregunta del usuario
- Historial conversacional
- Contexto de documentos (estructurado)

[OUTPUT SPECIFICATION]
- Formato exacto esperado
- JSON Schema expl√≠cito
- Ejemplos de output v√°lido (1-2)

[REASONING INSTRUCTIONS]
- C√≥mo abordar la tarea
- Chain-of-Thought si aplica
- Verificaciones de calidad
```

### 2. **Few-Shot Learning**
- **0-shot**: R√°pido pero inconsistente
- **1-shot**: +30% de mejora en consistencia
- **2-shot**: +50% de mejora, punto √≥ptimo
- **3+ shot**: Rendimientos marginales decrecientes

### 3. **JSON Schema Expl√≠cito**

En lugar de:
```
Devuelve JSON con esta estructura:
{ "campo": "valor" }
```

Usar:
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["resumen_ejecutivo", "respuesta_detallada"],
  "properties": {
    "resumen_ejecutivo": {
      "type": "string",
      "minLength": 20,
      "maxLength": 200,
      "description": "Resumen conciso de m√°ximo 200 caracteres"
    },
    "respuesta_detallada": {
      "type": "string",
      "description": "Respuesta completa en Markdown con citas [Doc N]"
    },
    ...
  }
}
```

### 4. **Chain-of-Thought para Tareas Complejas**

```
Antes de responder, reflexiona paso a paso:
1. ¬øQu√© informaci√≥n espec√≠fica necesito de los documentos?
2. ¬øQu√© documentos contienen esa informaci√≥n?
3. ¬øHay informaci√≥n conflictiva o complementaria?
4. ¬øC√≥mo estructuro la respuesta de manera l√≥gica?

<thinking>
[Tu proceso de razonamiento aqu√≠]
</thinking>

<answer>
[Respuesta final JSON]
</answer>
```

### 5. **Delimitadores Claros**

```xml
<system_context>
Eres Atenex, un asistente experto en an√°lisis de documentos corporativos.
</system_context>

<user_query>
{{ query }}
</user_query>

<document_context>
{% for doc in documents %}
<document id="{{ doc.id }}">
  <metadata>
    <filename>{{ doc.meta.file_name }}</filename>
    <page>{{ doc.meta.page }}</page>
  </metadata>
  <content>
    {{ doc.content }}
  </content>
</document>
{% endfor %}
</document_context>
```

### 6. **Control de Calidad Interno**

```
Antes de enviar tu respuesta, verifica:
- [ ] He usado SOLO informaci√≥n de los documentos proporcionados
- [ ] Todas las afirmaciones tienen su cita [Doc N] correspondiente
- [ ] El JSON generado es v√°lido y sigue el schema exacto
- [ ] La respuesta es completa y responde directamente la pregunta
```

---

## üöÄ PR√ìXIMOS PASOS

Ver el documento `plan_refactorizacion_prompts.md` para:
1. Plan detallado de implementaci√≥n
2. Nuevos templates de prompts optimizados
3. Estrategia de migraci√≥n
4. Testing y evaluaci√≥n
5. M√©tricas de √©xito

---

## üìà IMPACTO ESPERADO

| M√©trica | Actual | Despu√©s de Refactorizaci√≥n | Mejora |
|---------|--------|----------------------------|--------|
| Precisi√≥n de respuestas | ~75% | ~90% | +20% |
| Consistencia de formato JSON | ~85% | ~98% | +15% |
| Uso innecesario de MapReduce | ~40% | ~10% | -75% |
| Calidad de citas | ~70% | ~95% | +36% |
| Latencia promedio (Direct RAG) | 2.5s | 2.0s | -20% |
| Latencia promedio (MapReduce) | 8.5s | 5.5s | -35% |
| Tokens promedio consumidos | 35K | 28K | -20% |

---

**Autor:** An√°lisis T√©cnico - Query Service  
**Versi√≥n:** 1.0  
**Modelo Objetivo:** Gemini 2.5 Flash (1M context window)
